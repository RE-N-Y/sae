{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPProcessor, CLIPVisionModel, CLIPTextModel\n",
    "\n",
    "import torchvision\n",
    "from accelerate import Accelerator\n",
    "from autoencoder import SparseAutoencoder, TopkSparseAutoencoder\n",
    "from datalib import SafeTensorDataset\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"bf16\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "vit = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\", attn_implementation=\"sdpa\")\n",
    "text = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", attn_implementation=\"sdpa\")\n",
    "\n",
    "# choose the SAE to load\n",
    "name = \"cc3m-text-topk-lr-3e-4-k-4-expansion-4\"\n",
    "model = TopkSparseAutoencoder.from_pretrained(f\"RE-N-Y/{name}\")\n",
    "\n",
    "# run the model on sampled activations\n",
    "dataset = SafeTensorDataset(\"/data/cc3m/embeddings.safetensors\", \"text\")\n",
    "\n",
    "vit, text = torch.compile(vit), torch.compile(text)\n",
    "dataloader = dataset.dataloader(batch_size=1024, num_workers=96, drop_last=True)\n",
    "model, vit, dataloader = accelerator.prepare(model, vit, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# track topk inputs with highest activation for k-th hidden unit\n",
    "topk = 16\n",
    "pages = model.pages\n",
    "\n",
    "topk_values = torch.zeros(pages, topk, dtype=torch.bfloat16, device=\"cpu\")\n",
    "topk_idxes = torch.full((pages, topk), -1, dtype=torch.int64, device=\"cpu\")\n",
    "\n",
    "# online topk algorithm\n",
    "for bidx, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "    dictionary = model.encode(batch)\n",
    "\n",
    "    _topk_values, _topk_idxes = torch.topk(dictionary.T, k=topk, dim=-1, sorted=True)\n",
    "    _topk_idxes += bidx * 1024 # offset idxes to global idxes\n",
    "\n",
    "    _topk_idxes = _topk_idxes.detach().cpu()\n",
    "    _topk_values = _topk_values.detach().cpu()\n",
    "\n",
    "    topk_idxes = torch.where(_topk_values > topk_values, _topk_idxes, topk_idxes)\n",
    "    topk_values = torch.where(_topk_values > topk_values, _topk_values, topk_values)\n",
    "\n",
    "\n",
    "# now you can use topk_idxes to get topk inputs, images, texts, from the dataset..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sandbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
